{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import adam\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acousticness  danceability  duration  energy  instrumentalness  key  \\\n",
      "0         0.713         0.514    100125   0.521          0.816000    8   \n",
      "1         0.192         0.714    207019   0.614          0.000000    4   \n",
      "2         0.333         0.630    216200   0.455          0.000004    5   \n",
      "3         0.601         0.810    136413   0.221          0.210000    5   \n",
      "4         0.883         0.465    181440   0.459          0.000173    6   \n",
      "\n",
      "   liveness  loudness  mode  speechiness    tempo  time_signature  valence  \\\n",
      "0    0.1120   -14.835     0       0.0444  119.879               4    0.143   \n",
      "1    0.2630    -6.935     1       0.0319  123.969               4    0.582   \n",
      "2    0.1270    -9.290     1       0.0292  139.931               4    0.199   \n",
      "3    0.1840   -11.005     1       0.0429  109.960               4    0.798   \n",
      "4    0.0692    -8.137     0       0.0351   90.807               4    0.288   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "data = pd.read_csv('Data/training_data.csv');\n",
    "dataT = pd.read_csv('Data/songs_to_classify.csv');\n",
    "print(data.head());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.drop(columns=['label'])\n",
    "#X = data.drop(columns=['label','danceability','duration','key','instrumentalness','liveness','mode','tempo','time_signature','valence'])\n",
    "y_train = data['label']\n",
    "X_test = dataT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "#Normalizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#Feature Scaling\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "#Normalizer()\n",
    "#nm = Normalizer()\n",
    "#X_train = nm.fit_transform(X_train)\n",
    "#X_test = nm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intializing input layer and 1st hidden layer\n",
    "classifier.add(Dense(7, kernel_initializer ='lecun_uniform', activation ='relu',input_dim = 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding 2nd hidden layer\n",
    "classifier.add(Dense(7, kernel_initializer ='lecun_uniform', activation ='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add output layer\n",
    "classifier.add(Dense(1, kernel_initializer ='lecun_uniform', activation ='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile ANN Stochastic Gradient Method\n",
    "optimizer = adam(lr=0.001)\n",
    "classifier.compile(optimizer = optimizer,loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.7128 - acc: 0.5093\n",
      "Epoch 2/60\n",
      "750/750 [==============================] - 0s 338us/step - loss: 0.6315 - acc: 0.6813\n",
      "Epoch 3/60\n",
      "750/750 [==============================] - 0s 358us/step - loss: 0.5706 - acc: 0.7373\n",
      "Epoch 4/60\n",
      "750/750 [==============================] - 0s 406us/step - loss: 0.5192 - acc: 0.7693\n",
      "Epoch 5/60\n",
      "750/750 [==============================] - 0s 337us/step - loss: 0.4846 - acc: 0.7880\n",
      "Epoch 6/60\n",
      "750/750 [==============================] - 0s 333us/step - loss: 0.4611 - acc: 0.7827\n",
      "Epoch 7/60\n",
      "750/750 [==============================] - 0s 422us/step - loss: 0.4439 - acc: 0.7933\n",
      "Epoch 8/60\n",
      "750/750 [==============================] - 0s 336us/step - loss: 0.4318 - acc: 0.7973\n",
      "Epoch 9/60\n",
      "750/750 [==============================] - 0s 337us/step - loss: 0.4213 - acc: 0.8040\n",
      "Epoch 10/60\n",
      "750/750 [==============================] - 0s 476us/step - loss: 0.4133 - acc: 0.8093\n",
      "Epoch 11/60\n",
      "750/750 [==============================] - 0s 396us/step - loss: 0.4069 - acc: 0.8120\n",
      "Epoch 12/60\n",
      "750/750 [==============================] - 0s 340us/step - loss: 0.4016 - acc: 0.8093\n",
      "Epoch 13/60\n",
      "750/750 [==============================] - 0s 338us/step - loss: 0.3982 - acc: 0.8120\n",
      "Epoch 14/60\n",
      "750/750 [==============================] - 0s 399us/step - loss: 0.3917 - acc: 0.8133\n",
      "Epoch 15/60\n",
      "750/750 [==============================] - 0s 378us/step - loss: 0.3883 - acc: 0.8147\n",
      "Epoch 16/60\n",
      "750/750 [==============================] - 0s 318us/step - loss: 0.3845 - acc: 0.8173\n",
      "Epoch 17/60\n",
      "750/750 [==============================] - 0s 455us/step - loss: 0.3804 - acc: 0.8227\n",
      "Epoch 18/60\n",
      "750/750 [==============================] - 0s 348us/step - loss: 0.3764 - acc: 0.8280\n",
      "Epoch 19/60\n",
      "750/750 [==============================] - 0s 346us/step - loss: 0.3735 - acc: 0.8360\n",
      "Epoch 20/60\n",
      "750/750 [==============================] - 0s 346us/step - loss: 0.3701 - acc: 0.8387\n",
      "Epoch 21/60\n",
      "750/750 [==============================] - 0s 332us/step - loss: 0.3673 - acc: 0.8387\n",
      "Epoch 22/60\n",
      "750/750 [==============================] - 0s 329us/step - loss: 0.3654 - acc: 0.8413\n",
      "Epoch 23/60\n",
      "750/750 [==============================] - 0s 335us/step - loss: 0.3629 - acc: 0.8413\n",
      "Epoch 24/60\n",
      "750/750 [==============================] - 0s 342us/step - loss: 0.3610 - acc: 0.8467\n",
      "Epoch 25/60\n",
      "750/750 [==============================] - 0s 344us/step - loss: 0.3610 - acc: 0.8413\n",
      "Epoch 26/60\n",
      "750/750 [==============================] - 0s 326us/step - loss: 0.3577 - acc: 0.8453\n",
      "Epoch 27/60\n",
      "750/750 [==============================] - 0s 343us/step - loss: 0.3560 - acc: 0.8467\n",
      "Epoch 28/60\n",
      "750/750 [==============================] - 0s 346us/step - loss: 0.3552 - acc: 0.8507\n",
      "Epoch 29/60\n",
      "750/750 [==============================] - 0s 329us/step - loss: 0.3526 - acc: 0.8493\n",
      "Epoch 30/60\n",
      "750/750 [==============================] - 0s 327us/step - loss: 0.3515 - acc: 0.8493\n",
      "Epoch 31/60\n",
      "750/750 [==============================] - 0s 327us/step - loss: 0.3509 - acc: 0.8427\n",
      "Epoch 32/60\n",
      "750/750 [==============================] - 0s 343us/step - loss: 0.3487 - acc: 0.8507\n",
      "Epoch 33/60\n",
      "750/750 [==============================] - 0s 336us/step - loss: 0.3475 - acc: 0.8520\n",
      "Epoch 34/60\n",
      "750/750 [==============================] - 0s 341us/step - loss: 0.3471 - acc: 0.8520\n",
      "Epoch 35/60\n",
      "750/750 [==============================] - 0s 338us/step - loss: 0.3457 - acc: 0.8520\n",
      "Epoch 36/60\n",
      "750/750 [==============================] - 0s 319us/step - loss: 0.3441 - acc: 0.8493\n",
      "Epoch 37/60\n",
      "750/750 [==============================] - 0s 324us/step - loss: 0.3442 - acc: 0.8440\n",
      "Epoch 38/60\n",
      "750/750 [==============================] - 0s 314us/step - loss: 0.3434 - acc: 0.8507\n",
      "Epoch 39/60\n",
      "750/750 [==============================] - 0s 336us/step - loss: 0.3416 - acc: 0.8493\n",
      "Epoch 40/60\n",
      "750/750 [==============================] - 0s 336us/step - loss: 0.3412 - acc: 0.8547\n",
      "Epoch 41/60\n",
      "750/750 [==============================] - 0s 332us/step - loss: 0.3391 - acc: 0.8573\n",
      "Epoch 42/60\n",
      "750/750 [==============================] - 0s 318us/step - loss: 0.3390 - acc: 0.8613\n",
      "Epoch 43/60\n",
      "750/750 [==============================] - 0s 324us/step - loss: 0.3378 - acc: 0.8547\n",
      "Epoch 44/60\n",
      "750/750 [==============================] - 0s 327us/step - loss: 0.3373 - acc: 0.8560\n",
      "Epoch 45/60\n",
      "750/750 [==============================] - 0s 360us/step - loss: 0.3366 - acc: 0.8600\n",
      "Epoch 46/60\n",
      "750/750 [==============================] - 0s 336us/step - loss: 0.3349 - acc: 0.8600\n",
      "Epoch 47/60\n",
      "750/750 [==============================] - 0s 324us/step - loss: 0.3343 - acc: 0.8613\n",
      "Epoch 48/60\n",
      "750/750 [==============================] - 0s 329us/step - loss: 0.3332 - acc: 0.8613\n",
      "Epoch 49/60\n",
      "750/750 [==============================] - 0s 328us/step - loss: 0.3330 - acc: 0.8600\n",
      "Epoch 50/60\n",
      "750/750 [==============================] - 0s 337us/step - loss: 0.3311 - acc: 0.8600\n",
      "Epoch 51/60\n",
      "750/750 [==============================] - 0s 330us/step - loss: 0.3316 - acc: 0.8547\n",
      "Epoch 52/60\n",
      "750/750 [==============================] - 0s 316us/step - loss: 0.3314 - acc: 0.8587\n",
      "Epoch 53/60\n",
      "750/750 [==============================] - 0s 323us/step - loss: 0.3293 - acc: 0.8613\n",
      "Epoch 54/60\n",
      "750/750 [==============================] - 0s 332us/step - loss: 0.3287 - acc: 0.8587\n",
      "Epoch 55/60\n",
      "750/750 [==============================] - 0s 338us/step - loss: 0.3273 - acc: 0.8640\n",
      "Epoch 56/60\n",
      "750/750 [==============================] - 0s 318us/step - loss: 0.3257 - acc: 0.8613\n",
      "Epoch 57/60\n",
      "750/750 [==============================] - 0s 332us/step - loss: 0.3270 - acc: 0.8587\n",
      "Epoch 58/60\n",
      "750/750 [==============================] - 0s 318us/step - loss: 0.3253 - acc: 0.8653\n",
      "Epoch 59/60\n",
      "750/750 [==============================] - 0s 327us/step - loss: 0.3242 - acc: 0.8640\n",
      "Epoch 60/60\n",
      "750/750 [==============================] - 0s 322us/step - loss: 0.3227 - acc: 0.8640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa735c9e048>"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit ANN to training set\n",
    "classifier.fit(X_train,y_train,batch_size = 8,epochs = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter tuning\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(7, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=7, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=7, verbose=0)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.806667 using {'optimizer': 'RMSprop'}\n",
      "0.803333 (0.012472) with: {'optimizer': 'SGD'}\n",
      "0.806667 (0.023214) with: {'optimizer': 'RMSprop'}\n",
      "0.790000 (0.008165) with: {'optimizer': 'Adagrad'}\n",
      "0.793333 (0.027183) with: {'optimizer': 'Adadelta'}\n",
      "0.791667 (0.015456) with: {'optimizer': 'Adam'}\n",
      "0.796667 (0.020548) with: {'optimizer': 'Adamax'}\n",
      "0.761667 (0.024608) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=13, activation='softsign'))\n",
    "    model.add(Dense(10, activation='softsign'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    optimizer = adam(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.795000 using {'learn_rate': 0.001}\n",
      "0.795000 (0.029439) with: {'learn_rate': 0.001}\n",
      "0.750000 (0.024495) with: {'learn_rate': 0.01}\n",
      "0.788333 (0.027183) with: {'learn_rate': 0.1}\n",
      "0.756667 (0.024608) with: {'learn_rate': 0.2}\n",
      "0.725000 (0.035355) with: {'learn_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(init_mode='uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=13, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.805000 using {'init_mode': 'normal'}\n",
      "0.796667 (0.018409) with: {'init_mode': 'uniform'}\n",
      "0.778333 (0.006236) with: {'init_mode': 'lecun_uniform'}\n",
      "0.805000 (0.008165) with: {'init_mode': 'normal'}\n",
      "0.595000 (0.007071) with: {'init_mode': 'zero'}\n",
      "0.800000 (0.022730) with: {'init_mode': 'glorot_normal'}\n",
      "0.788333 (0.018856) with: {'init_mode': 'glorot_uniform'}\n",
      "0.803333 (0.022485) with: {'init_mode': 'he_normal'}\n",
      "0.790000 (0.021602) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=13, kernel_initializer='normal', activation='softsign', kernel_constraint=maxnorm(4)))\n",
    "    model.add(Dense(neurons, kernel_initializer='normal', activation='softsign', kernel_constraint=maxnorm(4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "neurons = [1,10,20,30,40,50,60,70,80,90,100,500,1000]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.805000 using {'neurons': 10}\n",
      "0.798333 (0.012472) with: {'neurons': 1}\n",
      "0.805000 (0.008165) with: {'neurons': 10}\n",
      "0.793333 (0.043269) with: {'neurons': 20}\n",
      "0.788333 (0.019293) with: {'neurons': 30}\n",
      "0.770000 (0.046368) with: {'neurons': 40}\n",
      "0.791667 (0.018409) with: {'neurons': 50}\n",
      "0.780000 (0.012247) with: {'neurons': 60}\n",
      "0.801667 (0.022485) with: {'neurons': 70}\n",
      "0.800000 (0.018708) with: {'neurons': 80}\n",
      "0.801667 (0.016997) with: {'neurons': 90}\n",
      "0.800000 (0.021602) with: {'neurons': 100}\n",
      "0.803333 (0.024608) with: {'neurons': 500}\n",
      "0.798333 (0.020950) with: {'neurons': 1000}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(7, input_dim=13, kernel_initializer='lecun_uniform', activation=activation))\n",
    "\tmodel.add(Dense(7, kernel_initializer='lecun_uniform', activation=activation))\n",
    "\tmodel.add(Dense(1, kernel_initializer='lecun_uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=7, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limbu/home/limbu/Python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "activation = ['softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.815000 using {'activation': 'hard_sigmoid'}\n",
      "0.806667 (0.006236) with: {'activation': 'softplus'}\n",
      "0.793333 (0.020138) with: {'activation': 'softsign'}\n",
      "0.805000 (0.004082) with: {'activation': 'relu'}\n",
      "0.793333 (0.019293) with: {'activation': 'tanh'}\n",
      "0.808333 (0.027183) with: {'activation': 'sigmoid'}\n",
      "0.815000 (0.032404) with: {'activation': 'hard_sigmoid'}\n",
      "0.813333 (0.018409) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MixPrediction\n",
    "ypred = classifier.predict(X_test)\n",
    "#Converting probabilities into 1 or 0  \n",
    "for i in range(0,200):\n",
    "    if ypred[i]>=.5:       # setting threshold to .5 \n",
    "        ypred[i]=1 \n",
    "    else: \n",
    "        ypred[i]=0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [200, 150]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-564-9d237eb648a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Confusion Matrix ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/limbu/Python/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/limbu/Python/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/home/limbu/Python/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 150]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score \n",
    "accuracy_1 = accuracy_score(ypred,y_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, ypred))\n",
    "print('\\n')\n",
    "print(\"===Accuracy===\")\n",
    "print(accuracy_1)\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, ypred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.][0.][0.][1.][0.][0.][1.][1.][1.][0.][1.][1.][0.][1.][1.][0.][1.][0.][1.][1.][0.][0.][1.][1.][0.][0.][1.][0.][0.][1.][1.][1.][0.][1.][1.][1.][1.][1.][0.][1.][0.][1.][0.][1.][0.][1.][0.][1.][1.][0.][0.][0.][1.][1.][0.][1.][1.][0.][0.][0.][1.][1.][1.][0.][0.][1.][1.][1.][1.][0.][1.][0.][1.][1.][1.][1.][0.][1.][1.][0.][1.][1.][0.][1.][0.][1.][1.][0.][0.][0.][0.][0.][0.][1.][1.][0.][1.][1.][1.][1.][1.][1.][1.][1.][0.][1.][0.][1.][1.][1.][1.][1.][0.][0.][1.][1.][1.][0.][1.][0.][0.][1.][1.][1.][1.][1.][1.][0.][1.][0.][1.][0.][1.][1.][1.][1.][1.][1.][1.][1.][1.][0.][1.][0.][1.][1.][0.][0.][1.][0.][1.][1.][0.][0.][1.][1.][1.][1.][1.][0.][1.][1.][0.][0.][1.][0.][1.][1.][1.][1.][1.][1.][1.][0.][0.][0.][0.][1.][1.][0.][0.][1.][1.][1.][1.][1.][1.][0.][1.][0.][0.][1.][1.][1.][1.][0.][0.][1.][1.][1.]\n"
     ]
    }
   ],
   "source": [
    "string = ''\n",
    "for i in ypred:\n",
    "    string += str(i)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
